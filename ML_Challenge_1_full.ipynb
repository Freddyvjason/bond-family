{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Machine Learning Battle Royale #1\n",
    "Welcome to the first Machine Learning NR Programming Battle Royale #1!!! (AKA: NR Programming Challenge # XX)\n",
    "\n",
    "<img src=\"files/ML_meme.jpeg\">\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "<b>Objective:</b> Train your neural network to recognize handwritten numbers to maximize the networks accuracy.\n",
    "\n",
    "<b>Input:</b>  Labeled handwritten number data from the mnist database. The input is in the form of 28x28 pixel gray scale picture of labeled data. \n",
    "\n",
    "<b>Output:</b> Trained network\n",
    "\n",
    "If you are new to neural networks the code below will walk you through the basics of and get your network training in no time.  If you already know what you are doing, go straight to the skeleton python notebook found here: <b>XXX</b>\n",
    "\n",
    "As always, happy coding!\n",
    "\n",
    "<img src = \"files/logo.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: The (Super) Basics\n",
    "\n",
    "In this programming challenge, we will train a classifier neural network to recognize handwritten numbers.  \n",
    "\n",
    "If you would like a much better explanation of AI feel free to reach out to Matt, Joe, or our resident AI TA, Sean. \n",
    "\n",
    "\n",
    "### 1. The Basic Building Block: Neuron\n",
    "\n",
    "The \"neuron\" is the basic building block of a neural network.  A <b>neuron</b>, as shown in the image, below consists of input numbers ($x_1,x_2,...,x_n$), weights ($w_1,w_2,...,w_n$), bias ($b$), and a non-linear activation function that produces an output ($y$).\n",
    "\n",
    "<img src=\"files/neuron.png\" width=\"400\">\n",
    "\n",
    "In the image above the weights are contained withing the red boxes, the bias is contained within the green box, and the non-linear activation function is within the yellow box. \n",
    "\n",
    "A few things are happening here. First, each input is multiplied by a weight:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_1 \\rightarrow x_1 * w_1\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "x_2 \\rightarrow x_2 * w_2\n",
    "\\end{equation*}\n",
    "\n",
    "Next, the weighted inputs are added to the bias (b):\n",
    "\n",
    "\\begin{equation*}\n",
    "(x_1*w_1)+(x_2*w_2)+b\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, the sum is passed through an activation function:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=f((x_1*w_1)+(x_2*w_2)+b)\n",
    "\\end{equation*}\n",
    "\n",
    "Activations functions are generally non-linear and serve to map the input values to the output values.  \n",
    "\n",
    "Common activation functions include the <b>sigmoid</b> function:\n",
    "\n",
    "<img src=\"files/sigmoid.png\" width=\"300\">\n",
    "\n",
    "You may be asking yourself, why do we even bother to use an activation function?  First, the sigmoid function is nonlinear.  This allows our neural network to learn non-linear functions.  For my fellow EE's out there, we are no longer bound to the ever present Linear Time Invariant system assumption (well, at least the linear part anyway)! \n",
    "\n",
    "Second, the system is bound to negative (0,1) as opposed (-inf,inf) with linear functions.  This means we do not have to worry about the output exploding into something unreasonable.  And for classifier examples (i.e. identifying if a picture is of a Cat or Not a Cat) the 0,1 output range serves as a good differentiator. \n",
    "\n",
    "There are other commonly used activation functions such as Rectified Linear Units (ReLu) and Hyperbolic Tangent (Tanh) that each have different strengths and weaknesses that will not be covered here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lets Build a Network: Combining Neurons\n",
    "\n",
    "While a single neuron allows us to \"learn\" some tasks, the real power comes when the single neurons are connected together to form a <b>neural network</b>.  A simple neural network is shown below:\n",
    "\n",
    "<img src=\"files/simple_network.png\" width=\"400\">\n",
    "\n",
    "A network consists of:\n",
    "- <b>Input Layer</b>: The input layer is the data ($x_1,x_2,...,x_n$)\n",
    "- <b>Hidden Layers</b>: Consists of n-layers of interconnected neurons. \n",
    "- <b>Output Layer</b>: The final layer of the network\n",
    "\n",
    "The simple network shown in the image above consists of a input layer, consisting of two input values which consists of a single integer of floating point number. A single hidden layer made of two neurons and an output layer consisting of one neuron. \n",
    "\n",
    "<b>Feedforward Example</b>\n",
    "\n",
    "We've created a simple network, now lets see what happens when we pass input values into out network.  Passing inputs to get an output is known as <b>feedforward</b>. \n",
    "\n",
    "$x_1$ and $x_2$ will be the inputs.  $h_1$, $h_2$ represent the output of our hidden layers, and $o_1$ represents the output of the network. \n",
    "\n",
    "Starting off, lets assume all neurons have the same weights $W=[0.5,1]$, the same bias $b=0$, and use the sigmoid activation function.  We will pass the input $X=[2,3]$ and step through to the output:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "h_1=h_2&=sigmoid(W \\cdot X+b)\\\\\n",
    "     &=sigmoid\\big((0.5*2)+(1*3)+0\\big)\\\\\n",
    "     &=sigmoid(4)\\\\\n",
    "     &=0.9820\\\\\n",
    "     \\\\\n",
    "o_1&=sigmoid(W \\cdot [h_1,h_2]+b)\\\\\n",
    "     &=sigmoid\\big((0.5*0.9820)+(1*0.9820)+0\\big)\\\\\n",
    "     &=sigmoid(1.473)\\\\\n",
    "     &=0.8135\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lets Get Training!\n",
    "\n",
    "We have made our network and calculated an output, but we have not done anything to make our network \"intelligent\".  To make out network have intelligence we need to train (update the neurons weights and biases) for the specific task.  For classifiers, such as our handwriting recognition task, networks train on input data with classified (labeled) outputs.  \n",
    "\n",
    "The network feedforwards the input and calculates outputs  for each sample.  The feedforwarded outputs for all the samples are compared to the classified data using a loss function.  The weights for each neuron are then updated to minimize the loss using a back propagation algorithm to update the weights and minimize the loss.  Congrats, we have made our network smarter!\n",
    "\n",
    "We will through a simple example for training a neural network and then jump into coding (this is the ML Battle Royale #1 after all).\n",
    "\n",
    "<b>Example</b>\n",
    "\n",
    "Lets assume you want to train your network to predict whether someone is an NR engineer.  Through your research, you identified two valuable data points at differentiating someone as an NR Engineer: someones competitiveness on a scale from 0 to 10, and number of board games they own.  You then forked over large sums of money to collect and clean the following exhaustive data set:\n",
    "\n",
    "<img src=\"files/engineer_table.png\" width=\"400\">\n",
    "\n",
    "And used your research funds to develop the following model:\n",
    "\n",
    "<img src=\"files/engineer_network.png\" width=\"400\">\n",
    "\n",
    "In your model NR Engineer is represented by a 1 and non-NR engineer is represented by a 0.  \n",
    "\n",
    "<b>Loss</b>\n",
    "\n",
    "As mentioned above, in order to train out network we need a measure of how good our function this.  In neural networks this is defined as the loss. \n",
    "\n",
    "One function is the <b>mean squared error</b> (MSE) loss:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{m} \\sum_{i=1}^{m}(y_{actual} - y_{pred})^2\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{m}$ is the number of samples\n",
    "- y the variable being predicted, which is NR Engineer\n",
    "- $y_{actual}$ is actual value of the prediction variable\n",
    "- $y_{pred}$ is the predicted value of the variable \n",
    "\n",
    "\n",
    "<b>Loss Example </b>\n",
    "\n",
    "Lets put the loss function to the test on our NR Engineer data.  For our first training run, lets say the network predicts 0 (Non-NR Engineer).  This would lead to the following loss calculation:\n",
    "\n",
    "<img src=\"files/engineer_mse.png\" width=\"300\">\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{4}(1 + 0 + 0 + 1) = 0.5\n",
    "\\end{equation}\n",
    "\n",
    "<b>Minimize the loss</b>\n",
    "\n",
    "We calculated the loss for the our prediction, now we must update weights and biases values to minimize the loss.\n",
    "\n",
    "We minimize the loss through <b>backpropagation</b>.  Backpropagation works by computing the gradient of the loss function for a single input-output combination.  Using the chain rule, the gradient can be computed for each weight ($w_1 - w_6$) and bias ($b_1 - b_3$) in the network. The weights are then updated based on a factor of the gradient called the <b>learning rate</b>.  \n",
    "This process is then repeated over and over until our network is meeting the desired performance.  If the network is reaching a plateau where it is making little to no progress no matter how much time you spend training it, it is time to evaluate either changing the model (size, learning rate, etc.), or gather more data. \n",
    "\n",
    "I will not go into the math here, but a quick google search will bring up plenty of derivations.\n",
    "\n",
    "I've skipped over a lot of details, but this description, and the codewalk through below should be enough to get you started on the programing challenge.  And as always, do not be shy about reaching out for help! \n",
    "\n",
    "Heavily Used Sources:\n",
    "\n",
    "Victor Zhou, <https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9>\n",
    "\n",
    "Wikipedia\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Challenge #1 Code Walkthrough\n",
    "\n",
    "As mentioned above the code below will walk you through making a hand writing tool network.\n",
    "\n",
    "<b> Grayscale Images </b>\n",
    "\n",
    "Before we begin lets review what goes into a grayscale image.  In a 8-bit grayscale image each pixel is represented with an integer between 0-255, where 0 is black and 255 is white.  The values in between represent varying shades of grade, from dark to light. \n",
    "\n",
    "An example grayscale image with the pixel values overlay is shown here:\n",
    "\n",
    "<img src = \"files/grayscale_example.png\">\n",
    "\n",
    "\n",
    "<b> Feed image into neural network </b>\n",
    "\n",
    "Now that we know a grayscale is nothing more than an array of integers from 0 -255, it is time to pass the information into our neural network.\n",
    "\n",
    "The first step is to unwrap the  mnist dataset (which if you remember from above is a 28x28 pixel image) into a single column.  The result is a 784x1 array as follows:\n",
    "\n",
    "<img src = \"files/unwrap.png\" width=400>\n",
    "\n",
    "Each row of the unwrapped array corresponds to a single pixel of the input image.\n",
    "\n",
    "The unwrapped array is then input into our neural network as shown here:\n",
    "\n",
    "<img src=\"files/image_ann.png\" width=400>\n",
    "\n",
    "where:\n",
    "\n",
    "- input layer: input for each pixel (1-784)\n",
    "- hidden layers: 2 hidden layers of size 2.  The number and hidden layers size for your training example are up to you.\n",
    "- output layer: Applies the softmax function to the output layer that is made up with 10 neurons; one for each digit, 0-9.\n",
    "\n",
    "<b> Softmax function</b> normalizes the probability of the output layer such that each component will be in the interval (0,1) and all of the components will add up to 1.  This allows to interpret the output as probabilities and make predictions based on the highest probabilities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Libraries</b> used in this challenge:\n",
    "- <b>numpy</b>: The fundamental package for scientific computing with Python. It includes functions for N-dimensional array objec, linear algebra, and random number capabilities.\n",
    "\n",
    "- <b>tensorflow</b>: An open-source software symbolic math library developed by Google that can be used generate neural networks.\n",
    "\n",
    "- <b>keras</b>: An open-source python library that is capable or running on top of tensorflow.  Keras allows you to define a nerual network at high-level of abstractions than tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libratries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<usepackage>\n",
    "<b>One Hot Encoding</b>\n",
    "To train our softmax output layer, we need to transform the labels, integers 0-9, into one hot encoding. One-hot is a group of bits where each bit represents a number.  0-9 are represented by an array length of 10.  Some examples of one hot encoding are shown here:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "0 = \\begin{bmatrix} 1\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0 \\end{bmatrix};\\quad\n",
    "2 = \\begin{bmatrix} 0\\\\0\\\\1\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0 \\end{bmatrix};\\quad\n",
    "9 = \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\1 \\end{bmatrix}\\quad\n",
    "\\end{equation*}\n",
    "\n",
    "The next function will one-hot encode our labels into an $mx10$ array and unwrap our input data into $mx784$ array, where m represents the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# function: create_onehot_classifier(y)\n",
    "#   Creates a one hot encoded array between (0,9) \n",
    "#\n",
    "# input: mx1 array of intergers between (0,9), \n",
    "#        where m represented the number of training examples\n",
    "#\n",
    "# output: mx10 array of one's hot encoded values\n",
    "############################################################\n",
    "\n",
    "\n",
    "def create_onehot_classifier(y):\n",
    "    \n",
    "    #Create and array zeros array in the shape of the final output\n",
    "    arr_train = np.zeros((y.shape[0],10),dtype=int) \n",
    "    \n",
    "    i=0 # Inititalize the index\n",
    "    \n",
    "    # Interate through each example and write that example to \n",
    "    #   the zeros array generated earlier \n",
    "    for val in y:\n",
    "        temp_arr = np.zeros(10)\n",
    "        temp_arr[0] = 1\n",
    "        temp_arr = np.roll(temp_arr, val)\n",
    "        arr_train[i,:] = temp_arr\n",
    "        i=i+1\n",
    "    return arr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block will load and format the training and test set:\n",
    "- <b>Training Set</b>:  The training set is the data used to train our model.\n",
    "- <b>Test Set</b>: The test set is the data used to evaluate the performance of th model. \n",
    "\n",
    "The reason we split the training and test sets is because we want to ensure our model is performing well on new data and helps evaluate if our model is <b>overfitting</b>.  Overfitting data occur when the network corresponds too closes to the training set and does not perform well on other datasets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28)\n",
      "y_train shape:  (60000,)\n",
      "x_test shape:  (10000, 28, 28)\n",
      "y_test shape:  (10000,)\n",
      "\n",
      "Formatted x_train shape: (60000, 784)\n",
      "Formatted y_train shape: (60000, 10)\n",
      "Formatted x_test shape: (10000, 784)\n",
      "Formatted y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# This block loads the data, and places it in the\n",
    "#    correct format for training\n",
    "\n",
    "#Load mnist data\n",
    "path = 'mnist.npz'\n",
    "with np.load(path) as data:\n",
    "  x_train_read = data['x_train']\n",
    "  y_train_read = data['y_train']\n",
    "  x_test_read = data['x_test']\n",
    "  y_test_read = data['y_test']\n",
    "\n",
    "# Print the shape of data of orginial data\n",
    "print(\"x_train shape: \", x_train_read.shape)\n",
    "print(\"y_train shape: \", y_train_read.shape)\n",
    "print(\"x_test shape: \", x_test_read.shape)\n",
    "print(\"y_test shape: \", y_test_read.shape)\n",
    "\n",
    "x_train = x_train_read.reshape(x_train_read.shape[0],-1)\n",
    "x_test = x_test_read.reshape(x_test_read.shape[0],-1)\n",
    "\n",
    "y_train = create_onehot_classifier(y_train_read)\n",
    "y_test = create_onehot_classifier(y_test_read)\n",
    "\n",
    "print(\"\\nFormatted x_train shape:\",x_train.shape)\n",
    "print(\"Formatted y_train shape:\",y_train.shape)\n",
    "print(\"Formatted x_test shape:\",x_test.shape)\n",
    "print(\"Formatted y_test shape:\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundation of any AI application is the data provided to the algorithm.  The cell below will load a random sample of the data.\n",
    "\n",
    "Rerun the cell multiple times to get familiar with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:  7\n",
      "y_train[16272] = [0 0 0 0 0 0 0 1 0 0]\n",
      "x_train[16272]:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMFklEQVR4nO3dX4hc5R3G8efRtgS0kag1TU1q2uhFS9GkhFiwFktpsblJctFiwJKCuF7UouBFxV6YSyn9Q68KW5SmpVWENjUXjW0IBQlIcZVoVkPqJmzjmjXboGiCiFV/vdhj2caZM5PzZ84kv+8Hhpl53zlzfhz22fecOTPndUQIwIXvoq4LADAahB1IgrADSRB2IAnCDiTxsVGuzDYf/QMtiwj3aq81stu+1fYR2zO276/zXgDa5arn2W1fLOmfkr4paU7SM5K2R8RLJcswsgMta2Nk3yRpJiKORcS7kh6TtKXG+wFoUZ2wXy3plSXP54q2/2N7wvaU7aka6wJQU50P6HrtKnxkNz0iJiVNSuzGA12qM7LPSVqz5PlqSSfqlQOgLXXC/oyk62x/zvYnJN0maU8zZQFoWuXd+Ih4z/bdkv4q6WJJj0TEi41VBqBRlU+9VVoZx+xA61r5Ug2A8wdhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IInK87NLku1ZSaclvS/pvYjY2ERRAJpXK+yFr0fEqQbeB0CL2I0Hkqgb9pD0N9vP2p7o9QLbE7anbE/VXBeAGhwR1Re2PxMRJ2xfJWmfpB9GxFMlr6++MgBDiQj3aq81skfEieJ+QdJuSZvqvB+A9lQOu+1LbH/yw8eSviVpuqnCADSrzqfxKyXttv3h+/whIp5spCoAjat1zH7OK+OYHWhdK8fsAM4fhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNHEBSfPC1u3bi3t3717d+X33rt3b2n/7Oxsaf/+/ftL+2dmZs61pAvCmTNnSvuPHj06okouDIzsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEmqvLXnvttaX9R44cGVElGNabb75Z2n/99deX9s/NzTVZznmDq8sCyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJpfs9+7Nix0v4bbrihtH/btm19+26//fbSZVesWFHaf8UVV5T2Z3XZZZeV9i9btmxElVwYBo7sth+xvWB7eknb5bb32X65uC//awbQuWF2438j6daz2u6XtD8irpO0v3gOYIwNDHtEPCXp9bOat0jaVTzeJan8mk8AOlf1mH1lRMxLUkTM276q3wttT0iaqLgeAA1p/QO6iJiUNCl1+0MYILuqp95O2l4lScX9QnMlAWhD1bDvkbSjeLxD0hPNlAOgLQN/z277UUm3SLpS0klJD0r6s6THJX1W0nFJ34mIsz/E6/VeKXfj165dW9q/bt260v6LLir/n3znnXeea0mNOX36dGl/2TXzd+7cWbrswkL5DuOGDRtK+1977bXS/gtVv9+zDzxmj4jtfbq+UasiACPF12WBJAg7kARhB5Ig7EAShB1IIs1PXLs0aMrmQf2D7Nu3r9bybdqyZUvlZQ8dOlTan/XUWlWM7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZ0ao1a9ZUXvb48eMNVgJGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsaNXmzZsrL7t3794GKwEjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAaG3fYjthdsTy9p22n7VdsHi1v1b04AGIlhRvbfSLq1R/svImJ9cftLs2UBaNrAsEfEU5JeH0EtAFpU55j9btsvFLv5K/q9yPaE7SnbUzXWBaCmqmH/laR1ktZLmpf0s34vjIjJiNgYERsrrgtAAyqFPSJORsT7EfGBpF9L2tRsWQCaVinstlctebpN0nS/1wIYDwN/z277UUm3SLrS9pykByXdYnu9pJA0K+muFmvEGFu+fHlp/zXXXNO375133ilddmZmplJN6G1g2CNie4/mh1uoBUCL+AYdkARhB5Ig7EAShB1IgrADSTgiRrcye3Qrw0isXbu2tP/o0aN9+06dOlW67MqVK6uUlF5EuFc7IzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kMfDqskBbDhw40HUJqTCyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdHZ6anp7suIZWBI7vtNbb/bvuw7Rdt31O0X257n+2Xi/sV7ZcLoKphduPfk3RfRHxB0lck/cD2FyXdL2l/RFwnaX/xHMCYGhj2iJiPiOeKx6clHZZ0taQtknYVL9slaWtbRQKo75yO2W2vlbRB0j8krYyIeWnxH4Ltq/osMyFpol6ZAOoaOuy2L5X0R0n3RsRbds+54z4iIiYlTRbvwcSOQEeGOvVm++NaDPrvI+JPRfNJ26uK/lWSFtopEUATBo7sXhzCH5Z0OCJ+vqRrj6Qdkh4q7p9opUKMtRtvvLHrEjCkYXbjb5L0PUmHbB8s2h7QYsgft32HpOOSvtNOiQCaMDDsEXFAUr8D9G80Ww6AtvB1WSAJwg4kQdiBJAg7kARhB5LgJ66oZf369ZWXnZ2dba4QDMTIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJ4dtSxbtqzysk8++WSDlWAQRnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSMIRo5ukhRlhLjxvvPFGaf/y5cv79q1evbp02fn5+Uo1ZRcRPa8GzcgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kMMz/7Gkm/lfRpSR9ImoyIX9reKelOSf8uXvpARPylrUIxnp5//vnS/ptvvnlElWCQYS5e8Z6k+yLiOduflPSs7X1F3y8i4qftlQegKcPMzz4vab54fNr2YUlXt10YgGad0zG77bWSNkj6R9F0t+0XbD9ie0WfZSZsT9meqlUpgFqGDrvtSyX9UdK9EfGWpF9JWidpvRZH/p/1Wi4iJiNiY0RsbKBeABUNFXbbH9di0H8fEX+SpIg4GRHvR8QHkn4taVN7ZQKoa2DYbVvSw5IOR8TPl7SvWvKybZKmmy8PQFOG+TT+Jknfk3TI9sGi7QFJ222vlxSSZiXd1UqFGGtPP/10aT+n3sbHMJ/GH5DU6/exnFMHziN8gw5IgrADSRB2IAnCDiRB2IEkCDuQBFM2o5aXXnqptH92drZv39tvv91wNSjDyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSYx6yuZ/S/rXkqYrJZ0aWQHnZlxrG9e6JGqrqsnaromIT/XqGGnYP7Jye2pcr003rrWNa10StVU1qtrYjQeSIOxAEl2HfbLj9ZcZ19rGtS6J2qoaSW2dHrMDGJ2uR3YAI0LYgSQ6CbvtW20fsT1j+/4uaujH9qztQ7YPdj0/XTGH3oLt6SVtl9veZ/vl4r7nHHsd1bbT9qvFtjtoe3NHta2x/Xfbh22/aPueor3TbVdS10i228iP2W1fLOmfkr4paU7SM5K2R0T5VRBGxPaspI0R0fkXMGx/TdIZSb+NiC8VbT+R9HpEPFT8o1wRET8ak9p2SjrT9TTexWxFq5ZOMy5pq6Tvq8NtV1LXdzWC7dbFyL5J0kxEHIuIdyU9JmlLB3WMvYh4StLrZzVvkbSreLxLi38sI9entrEQEfMR8Vzx+LSkD6cZ73TbldQ1El2E/WpJryx5Pqfxmu89JP3N9rO2J7oupoeVETEvLf7xSLqq43rONnAa71E6a5rxsdl2VaY/r6uLsPeaSmqczv/dFBFflvRtST8odlcxnKGm8R6VHtOMj4Wq05/X1UXY5yStWfJ8taQTHdTRU0ScKO4XJO3W+E1FffLDGXSL+4WO6/mfcZrGu9c04xqDbdfl9OddhP0ZSdfZ/pztT0i6TdKeDur4CNuXFB+cyPYlkr6l8ZuKeo+kHcXjHZKe6LCW/zMu03j3m2ZcHW+7zqc/j4iR3yRt1uIn8kcl/biLGvrU9XlJzxe3F7uuTdKjWtyt+48W94jukHSFpP2SXi7uLx+j2n4n6ZCkF7QYrFUd1fZVLR4aviDpYHHb3PW2K6lrJNuNr8sCSfANOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1I4r+/IL+KazyxLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the Data\n",
    "# Rerun this cell to load additonal examples\n",
    "\n",
    "# Integer between 0-59,999\n",
    "index = np.random.randint(0,59999)\n",
    "plt.imshow(x_train_read[index], cmap='gray')\n",
    "str_yout = 'y_train['+str(index)+'] ='\n",
    "str_xout = 'x_train['+str(index)+']:'\n",
    "print(\"Number: \", y_train_read[index])\n",
    "print(str_yout,y_train[index])\n",
    "print(str_xout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is finally time to make your neural network!  This block will use keras to define and load your model. \n",
    "\n",
    "As discussed above.  The default model has 2 hidden layers each made of 2 neurons and a softmax output layer of size 10. \n",
    "\n",
    "The number of hidden layers and number of neurons can be adjusted to train and improve model performance.  You will need to iterate to find which combination provides the best accuracy and balance the amount of time it takes to train the model (In general, larger model takes more computational power to train). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis block contains the function that \n",
    "\n",
    "def create_model():\n",
    "   \n",
    "    # Hyper Parameters\n",
    "    # Tune these to change perfromance\n",
    "    learning_rate = 0.0005\n",
    "    hidden_layer1_size = 2 # number of neurons in hidden layer 1\n",
    "    hidden_layer2_size = 2 # number of neurons in hidden layer 2\n",
    "    \n",
    "    # Block defining neural network \n",
    "    # Add/Delete layers to change performance \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(hidden_layer1_size, activation=tf.nn.relu, input_shape = (784,)),\n",
    "        keras.layers.Dense(hidden_layer2_size, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
    "    ])\n",
    "\n",
    "    # By default will use:\n",
    "    #   Adams Optimizer with learning rate 0.0005\n",
    "    #   Mean Square Error Loss Function\n",
    "    #   Metrics, which does is only provided for information and not used \n",
    "    #      in training, is model accuracy on traing set in decimal form. \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss=tf.keras.losses.mean_squared_error,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load the neural network \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created the model, now it is to train the model. \n",
    "\n",
    "We will use the .fit method to fit the model based on x_train and y_train input. \n",
    "\n",
    "The methods include:\n",
    "\n",
    "- <b>epochs</b> is the number of times the model will run through all m (60000) training examples.\n",
    "\n",
    "- batch-size is the size of the <b>mini-batch</b>, or number of samples per loss calculation that is then used to update the network weights.  Mini-batches speed up the learning of your network rather than calculating the loss and updating the weights for all 60,000 samples at once. You can use any batch-size that you want, but choosing a factor of 2 will improve computation speed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0828 - accuracy: 0.1983\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0710 - accuracy: 0.3225\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0597 - accuracy: 0.4536\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0572 - accuracy: 0.4909\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0558 - accuracy: 0.5006\n",
      "Epoch 6/100\n",
      "   32/60000 [..............................] - ETA: 5s - loss: 0.0584 - accuracy: 0.4062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-318d462d6f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model using the .fit command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    124\u001b[0m       \u001b[0mcurrent_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     with training_context.on_batch(\n\u001b[1;32m--> 126\u001b[1;33m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0m\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[1;34m(self, step, mode, size)\u001b[0m\n\u001b[0;32m    779\u001b[0m       \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m       self.callbacks._call_batch_hook(\n\u001b[1;32m--> 781\u001b[1;33m           mode, 'begin', step, batch_logs)\n\u001b[0m\u001b[0;32m    782\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[0;32m    244\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3495\u001b[0m     \"\"\"\n\u001b[0;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3497\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3405\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3406\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3528\u001b[0m             \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3529\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3530\u001b[1;33m         \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model using the .fit command\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 100, batch_size = 32,initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.3777\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test,verbose = 'False')\n",
    "print(\"Test Accuracy = \",  test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:            6\n",
      "Model Prediction:  0\n",
      "x_train[16272]:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let Look at how the model perfromed\n",
    "# Rerun the model to figure which test cases your model\n",
    "#    predicted right and which ones it predicted wrong\n",
    "# \n",
    "# Evaluating which data your model does well on which ones\n",
    "#   need improvemt can focus your data collection and traing efforts \n",
    "\n",
    "index = np.random.randint(0,x_train.shape[0])\n",
    "plt.imshow(x_train_read[index], cmap='gray')\n",
    "\n",
    "xin = x_train[index].reshape(-1,1)\n",
    "xin = xin.T\n",
    "arr_predict = model.predict(xin,verbose = 'False')\n",
    "# transform array to number\n",
    "result = np.argmax(arr_predict)\n",
    "\n",
    "print(\"Number:           \", y_train_read[index])\n",
    "print(\"Model Prediction: \", result)\n",
    "print(str_xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to save your model\n",
    "# This file will be the submission to the programming challenge\n",
    "model.save('NAME_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
